# -*- coding: utf-8 -*-
"""BOT

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/144eaP6BtQP2A6SoLwBWijh9Au8knkOFm
"""



"""# Task
Create a Google Colab notebook that sets up and runs a free, highly interactive, and advanced talking bot for customer service using the most capable free and open-source large language model runnable efficiently within Google Colab's typical resource limits.

## Setup environment

### Subtask:
Install necessary libraries and dependencies (e.g., `transformers`, `accelerate`, `bitsandbytes`, `gradio`).

**Reasoning**:
Install the required libraries using pip.
"""

!pip install transformers accelerate bitsandbytes gradio

"""## Model selection and loading

### Subtask:
Choose a suitable free and open-source LLM (like a quantized version of Llama 3 8B or Mistral 7B) that can run efficiently in Colab and load it using `transformers`.

**Reasoning**:
Import the necessary class and load a suitable quantized model using `from_pretrained` with appropriate arguments for Colab efficiency.

# Task
Create a Google Colab notebook that sets up and runs a free, highly interactive, and advanced talking bot for customer service using the most capable free and open-source large language model available that can be run efficiently within Google Colab's typical resource limits (e.g., a quantized version of Llama 3 8B, Mistral 7B, or similar, prioritizing performance and quality). The notebook should include model and tokenizer loading, pipeline setup, and a Gradio interface for interaction.

## Model selection and loading

### Subtask:
Choose a suitable free and open-source LLM (like a quantized version of Llama 3 8B or Mistral 7B) that can run efficiently in Colab and load it using `transformers`.

**Reasoning**:
Import the necessary class and load a suitable quantized model using `from_pretrained` with appropriate arguments for Colab efficiency.
"""

from transformers import AutoModelForCausalLM

model_id = "meta-llama/Meta-Llama-3-8B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    load_in_4bit=True,
    torch_dtype="auto",
    trust_remote_code=True,
)

"""**Reasoning**:
The previous attempt to load the model failed because it was a gated model requiring authentication. I will choose an alternative, readily available open-source model that is suitable for Colab and try loading it again.


"""

from transformers import AutoModelForCausalLM

model_id = "NousResearch/Llama-3-8B-Instruct-DPO" # An alternative, freely available model
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    load_in_4bit=True,
    torch_dtype="auto",
    trust_remote_code=True,
)

"""**Reasoning**:
The previous model ID was incorrect. I need to find a publicly available, 4-bit quantized Llama 3 8B model on Hugging Face Hub that is known to work well on Colab and try loading that.


"""

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model_id = "unsloth/llama-3-8b-Instruct-bnb-4bit" # A readily available 4-bit quantized model
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    torch_dtype=torch.float16,
    load_in_4bit=True,
    trust_remote_code=True,
)

"""## Tokenizer loading

### Subtask:
Load the corresponding tokenizer for the selected model.

**Reasoning**:
Load the tokenizer for the chosen model.
"""

tokenizer = AutoTokenizer.from_pretrained(model_id)

"""## Pipeline setup

### Subtask:
Set up a text generation pipeline using `transformers`.

**Reasoning**:
Import the `pipeline` function and create a text generation pipeline using the loaded model and tokenizer.
"""

from transformers import pipeline

pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)

"""## Gradio interface creation

### Subtask:
Create a simple Gradio interface for the chatbot.

**Reasoning**:
Define the Gradio interface components and the chatbot response function using the previously loaded model and tokenizer.
"""

import gradio as gr

def chatbot_response(message, history):
    # Format the input for the model (adjust based on the model's expected format)
    prompt = f"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n{message}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n"
    response = pipe(prompt, max_new_tokens=256)[0]['generated_text']

    # Extract only the assistant's response
    # This is a simple approach; more robust parsing might be needed depending on the model's output
    response_start = response.find("<|start_header_id|>assistant<|end_header_id|>")
    if response_start != -1:
        response = response[response_start + len("<|start_header_id|>assistant<|end_header_id|>") + 1:].strip()

    return response

# Define the Gradio interface
iface = gr.Interface(
    fn=chatbot_response,
    inputs=gr.Textbox(lines=2, placeholder="Enter your message here..."),
    outputs="textbox",
    title="Customer Service Chatbot",
    description="Interact with a free and open-source LLM for customer service.",
    examples=[["What is your return policy?"], ["How can I track my order?"]]
)

"""**Reasoning**:
The previous code block generated a warning because the `chatbot_response` function is expected to take two arguments (`message` and `history`) when used with Gradio's chatbot component, but the interface was set up for a single input textbox. The subtask is to create a simple Gradio interface, which should include handling chat history. I will modify the function signature and the Gradio interface to use the `gr.ChatInterface` which is designed for chatbot applications and automatically handles history.


"""

import gradio as gr

def chatbot_response(message, history):
    # The history is a list of tuples, each tuple is (user_message, bot_response)
    # We need to format the entire conversation history for the model
    conversation = "".join([f"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n{user}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n{bot}<|eot_id|>" for user, bot in history])
    conversation += f"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n{message}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n"

    response = pipe(conversation, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)[0]['generated_text']

    # Extract only the assistant's response from the potentially longer generated text
    response_start = response.rfind("<|start_header_id|>assistant<|end_header_id|>")
    if response_start != -1:
        response = response[response_start + len("<|start_header_id|>assistant<|end_header_id|>") + 1:].strip()
        # Remove any trailing end-of-turn or end-of-text tokens if the model generated them
        response = response.split("<|eot_id|>")[0].strip()

    return response

# Define the Gradio ChatInterface
iface = gr.ChatInterface(
    fn=chatbot_response,
    title="Customer Service Chatbot",
    description="Interact with a free and open-source LLM for customer service.",
    examples=[["What is your return policy?"], ["How can I track my order?"]]
)

"""## Run the gradio app

### Subtask:
Launch the Gradio app.

**Reasoning**:
Launch the Gradio interface to make the chatbot accessible.
"""

iface.launch(share=True)

"""## Summary:

### Data Analysis Key Findings

*   A free and open-source language model suitable for Google Colab's resources, specifically `unsloth/llama-3-8b-Instruct-bnb-4bit`, was successfully loaded using 4-bit quantization.
*   The corresponding tokenizer for the selected model was loaded.
*   A text generation pipeline was successfully set up using the loaded model and tokenizer, configured to utilize the available GPU.
*   A Gradio `ChatInterface` was created, including a function to handle conversation history and generate responses using the text generation pipeline.
*   The Gradio application was launched, providing a public URL for interaction with the customer service chatbot.

### Insights or Next Steps

*   The current implementation extracts the assistant's response by searching for a specific header and splitting on end-of-turn tokens. Further refinement of the output parsing logic could improve robustness against unexpected model output formats.
*   Exploring techniques like prompt engineering or fine-tuning on customer service dialogues could enhance the chatbot's performance and relevance for specific customer service scenarios.

"""